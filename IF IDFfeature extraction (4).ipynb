{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ddc604a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fcd2947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1b8caef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1123757263427186690</td>\n",
       "      <td>hate wen females hit ah nigga with tht bro 😂😂,...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1123733301397733380</td>\n",
       "      <td>RT @airjunebug: When you're from the Bay but y...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1123734094108659712</td>\n",
       "      <td>RT @DonaldJTrumpJr: Dear Democrats: The Americ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1126951188170199049</td>\n",
       "      <td>RT @SheLoveTimothy: He ain’t on drugs he just ...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1126863510447710208</td>\n",
       "      <td>RT @TavianJordan: Summer ‘19 I’m coming for yo...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1126798721025544193</td>\n",
       "      <td>RT @prodnose: Good morning, everyone.\\nFollowi...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1126833089190219777</td>\n",
       "      <td>@cheezitking123 this what you get for tryna ge...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1130037092845670400</td>\n",
       "      <td>earphones ko 😭😭😭😭😭😭😭</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1127028455651123201</td>\n",
       "      <td>RT @nj_linguist: @realgonegirl @elivalley I th...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1130285076858789889</td>\n",
       "      <td>i’m tired as fuck. and man, physically ain’t S...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id                                               text  \\\n",
       "0    1123757263427186690  hate wen females hit ah nigga with tht bro 😂😂,...   \n",
       "1    1123733301397733380  RT @airjunebug: When you're from the Bay but y...   \n",
       "2    1123734094108659712  RT @DonaldJTrumpJr: Dear Democrats: The Americ...   \n",
       "3    1126951188170199049  RT @SheLoveTimothy: He ain’t on drugs he just ...   \n",
       "4    1126863510447710208  RT @TavianJordan: Summer ‘19 I’m coming for yo...   \n",
       "..                   ...                                                ...   \n",
       "995  1126798721025544193  RT @prodnose: Good morning, everyone.\\nFollowi...   \n",
       "996  1126833089190219777  @cheezitking123 this what you get for tryna ge...   \n",
       "997  1130037092845670400                               earphones ko 😭😭😭😭😭😭😭   \n",
       "998  1127028455651123201  RT @nj_linguist: @realgonegirl @elivalley I th...   \n",
       "999  1130285076858789889  i’m tired as fuck. and man, physically ain’t S...   \n",
       "\n",
       "    task1  \n",
       "0     HOF  \n",
       "1     HOF  \n",
       "2     NOT  \n",
       "3     HOF  \n",
       "4     NOT  \n",
       "..    ...  \n",
       "995   NOT  \n",
       "996   NOT  \n",
       "997   NOT  \n",
       "998   NOT  \n",
       "999   HOF  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('sample_text - Sheet1 (1).csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "49c6b872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emot in c:\\users\\user\\anaconda3\\lib\\site-packages (3.1)\n",
      "Requirement already satisfied: emoji in c:\\users\\user\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: demoji in c:\\users\\user\\anaconda3\\lib\\site-packages (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement punct (from versions: none)\n",
      "ERROR: No matching distribution found for punct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\user\\anaconda3\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\user\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\user\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\user\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install emot\n",
    "!pip install emoji\n",
    "!pip install demoji\n",
    "!pip install punct\n",
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fc77da54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "255aabce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e42e162a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_860/1435932555.py:23: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    }
   ],
   "source": [
    "#from textblob import TextBlob\n",
    "import emot\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import emoji\n",
    "import demoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import regex\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "demoji.download_codes()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "226964c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1123757263427186690</td>\n",
       "      <td>hate wen females hit ah nigga with tht bro 😂😂,...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1123733301397733380</td>\n",
       "      <td>RT @airjunebug: When you're from the Bay but y...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1123734094108659712</td>\n",
       "      <td>RT @DonaldJTrumpJr: Dear Democrats: The Americ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1126951188170199049</td>\n",
       "      <td>RT @SheLoveTimothy: He ain’t on drugs he just ...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1126863510447710208</td>\n",
       "      <td>RT @TavianJordan: Summer ‘19 I’m coming for yo...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1123757263427186690  hate wen females hit ah nigga with tht bro 😂😂,...   \n",
       "1  1123733301397733380  RT @airjunebug: When you're from the Bay but y...   \n",
       "2  1123734094108659712  RT @DonaldJTrumpJr: Dear Democrats: The Americ...   \n",
       "3  1126951188170199049  RT @SheLoveTimothy: He ain’t on drugs he just ...   \n",
       "4  1126863510447710208  RT @TavianJordan: Summer ‘19 I’m coming for yo...   \n",
       "\n",
       "  task1  \n",
       "0   HOF  \n",
       "1   HOF  \n",
       "2   NOT  \n",
       "3   HOF  \n",
       "4   NOT  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=data['text']\n",
    "y=data['task1']\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b1941918",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3ff93d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253    RT @YoungTheGoat: Nigga Said “Wyd” niggas are ...\n",
       "667    RT @Saintsfan5348: @RepJimLower @realDonaldTru...\n",
       "85     @CHIMPSINSOCKS And all the other complete and ...\n",
       "969    Bitches be pressed as fuuuuuck!!!!! You don't ...\n",
       "75     RT @bilalfqi: While Shias are protesting again...\n",
       "                             ...                        \n",
       "835    Been a while since I had a little Willie in my...\n",
       "192    RT @OgbeniDipo: To my young followers, please ...\n",
       "629                 @wolfrad_hentai That’s a lotta ass 👀\n",
       "559    RT @ggukreum: GET THE DAMN GETTY IMAGES WATERM...\n",
       "684    RT @uthman_waxcav: You're making mouth all ove...\n",
       "Name: text, Length: 750, dtype: object"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5ebf29ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "993    RT @OfficialChip: There will never be another ...\n",
       "859    RT @taeilrnb: flo taeil shit, bitch ! https://...\n",
       "298    @MiguelSantosIII @tamwelsh Yes. Unfortunately ...\n",
       "553    RT @GGYounggBoy: I rather be single then stupi...\n",
       "672    Not getting any sleep tonight, will be time fo...\n",
       "                             ...                        \n",
       "462    RT @frankwarren_tv: “Congratulations @BronzeBo...\n",
       "356    RT @philsadelphia: in honor of paul rudd hosti...\n",
       "2      RT @DonaldJTrumpJr: Dear Democrats: The Americ...\n",
       "478    RT @simpboylen: might fuck around and get myse...\n",
       "695    When the AirBnB has banging mirrors all round ...\n",
       "Name: text, Length: 250, dtype: object"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0633b67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253    HOF\n",
       "667    HOF\n",
       "85     HOF\n",
       "969    HOF\n",
       "75     NOT\n",
       "      ... \n",
       "835    NOT\n",
       "192    NOT\n",
       "629    HOF\n",
       "559    HOF\n",
       "684    NOT\n",
       "Name: task1, Length: 750, dtype: object"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "10b97bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "993    HOF\n",
       "859    HOF\n",
       "298    HOF\n",
       "553    HOF\n",
       "672    NOT\n",
       "      ... \n",
       "462    NOT\n",
       "356    NOT\n",
       "2      NOT\n",
       "478    HOF\n",
       "695    HOF\n",
       "Name: task1, Length: 250, dtype: object"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "470269fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HOF    501\n",
       "NOT    499\n",
       "Name: task1, dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['task1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9c39501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emot_object = emot.core.emot()\n",
    "ps =PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "english_stopwords = stopwords.words('english')\n",
    "exclude = set(string.punctuation)\n",
    "def preprocess(text):\n",
    "  #text=demoji.findall(df['Text'])\n",
    "    text = contractions.fix(text.lower(), slang=True)\n",
    "    text =re.sub(\"@ ?[A-Za-z0-9_]+\", \"\", text)\n",
    "    text= re.sub(r'\\d+', '', text)\n",
    "    text=re.sub(r'$', '', text)\n",
    "    text= re.sub(r'’','', text )\n",
    "    text=re.sub('<.*?>','',text)\n",
    "    text=re.sub(r'http\\S+', '', text)\n",
    "  #text=emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    text = ''.join(ch for ch in text if ch not in exclude)\n",
    "    tokens = word_tokenize(text)\n",
    "  #print(\"Tokens:\", tokens)\n",
    "    text = [t for t in tokens if t not in english_stopwords]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e27b53b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "#import demoji\n",
    "#demoji.download_codes()\n",
    "def emo(text):\n",
    "    \n",
    "    temp=emoji.demojize(text,delimiters=(\" \",\" \"))\n",
    "    temp=temp.replace(\"_\",\"  \")\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "efaa3309",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text']=data[\"text\"].apply(lambda x:emo(x))\n",
    "data[\"clean_text\"]=data['clean_text'].apply(lambda X: preprocess(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ab28a9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.count of                 tweet_id                                               text  \\\n",
       "0    1123757263427186690  hate wen females hit ah nigga with tht bro 😂😂,...   \n",
       "1    1123733301397733380  RT @airjunebug: When you're from the Bay but y...   \n",
       "2    1123734094108659712  RT @DonaldJTrumpJr: Dear Democrats: The Americ...   \n",
       "3    1126951188170199049  RT @SheLoveTimothy: He ain’t on drugs he just ...   \n",
       "4    1126863510447710208  RT @TavianJordan: Summer ‘19 I’m coming for yo...   \n",
       "..                   ...                                                ...   \n",
       "995  1126798721025544193  RT @prodnose: Good morning, everyone.\\nFollowi...   \n",
       "996  1126833089190219777  @cheezitking123 this what you get for tryna ge...   \n",
       "997  1130037092845670400                               earphones ko 😭😭😭😭😭😭😭   \n",
       "998  1127028455651123201  RT @nj_linguist: @realgonegirl @elivalley I th...   \n",
       "999  1130285076858789889  i’m tired as fuck. and man, physically ain’t S...   \n",
       "\n",
       "    task1                                         clean_text  \n",
       "0     HOF  hate wen females hit ah nigga tht bro face tea...  \n",
       "1     HOF                     rt bay really ny nigga heart w  \n",
       "2     NOT  rt dear democrats american people stupid know ...  \n",
       "3     HOF           rt drugs bored shit bored face tears joy  \n",
       "4     NOT  rt summer ‘ coming boring shit beach days road...  \n",
       "..    ...                                                ...  \n",
       "995   NOT  rt good morning everyone following one worst d...  \n",
       "996   NOT  get tryna get kfc expressionless face expressi...  \n",
       "997   NOT  earphones ko loudly crying face loudly crying ...  \n",
       "998   NOT  rt linguist think people need realize art ever...  \n",
       "999   HOF  tired fuck man physically shit mentally draine...  \n",
       "\n",
       "[1000 rows x 4 columns]>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2b0c5ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import regex\n",
    "\n",
    "def custom_analyzer(text):\n",
    "    words = regex.findall(r'\\w{2,}', text) # extract words of at least 2 letters\n",
    "    for w in words:\n",
    "        yield w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97467e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without parameter passing......\n",
    "\n",
    "tf_idf = CountVectorizer()\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.fit_transform(data['clean_text'])\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.transform(data['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0acdb3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without parameter passing......\n",
    "\n",
    "tf_idf = CountVectorizer(analyzer='char', ngram_range=(2, 2))\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.fit_transform(data['clean_text'])\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.transform(data['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5f50382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without parameter passing......\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.fit_transform(data['clean_text'])\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.transform(data['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "164ced58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with parameters\n",
    "\n",
    "tf_idf = TfidfVectorizer(analyzer='char',ngram_range=(1,3),max_df=1.0,min_df=1,max_features=5000)\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.fit_transform(data['clean_text'])\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.transform(data['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e7203682",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer()\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.fit_transform(data['clean_text'])\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.transform(data['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d2c7f64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x693 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 42841 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4fec8194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming test data into tf-idf matrix\n",
    "X_test_tf = tf_idf.transform(data[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "42d0d2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x693 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 42841 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "13ad820b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "lsvc = LinearSVC()\n",
    "lsvc.fit(X_train_tf, data['task1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8ca97817",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lsvc.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b95409a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF',\n",
       "       'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT',\n",
       "       'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF',\n",
       "       'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF',\n",
       "       'NOT', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'NOT',\n",
       "       'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT',\n",
       "       'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF',\n",
       "       'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF',\n",
       "       'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT',\n",
       "       'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT',\n",
       "       'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF',\n",
       "       'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF',\n",
       "       'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF',\n",
       "       'HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF',\n",
       "       'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF',\n",
       "       'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'NOT',\n",
       "       'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT',\n",
       "       'NOT', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT',\n",
       "       'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF',\n",
       "       'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'HOF',\n",
       "       'HOF', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT',\n",
       "       'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT',\n",
       "       'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF',\n",
       "       'NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'NOT',\n",
       "       'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF',\n",
       "       'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT',\n",
       "       'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF',\n",
       "       'HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF',\n",
       "       'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF',\n",
       "       'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF',\n",
       "       'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF',\n",
       "       'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF',\n",
       "       'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT',\n",
       "       'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF',\n",
       "       'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'NOT',\n",
       "       'HOF', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT',\n",
       "       'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'HOF',\n",
       "       'NOT', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF',\n",
       "       'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF',\n",
       "       'NOT', 'NOT', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF',\n",
       "       'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'NOT',\n",
       "       'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF',\n",
       "       'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT',\n",
       "       'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'HOF',\n",
       "       'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'HOF', 'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT', 'NOT',\n",
       "       'HOF', 'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'NOT',\n",
       "       'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT',\n",
       "       'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT',\n",
       "       'HOF', 'HOF', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF',\n",
       "       'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF',\n",
       "       'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF', 'NOT',\n",
       "       'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'NOT', 'HOF', 'HOF', 'NOT', 'HOF', 'NOT', 'HOF',\n",
       "       'NOT', 'HOF', 'HOF', 'NOT', 'NOT', 'HOF', 'HOF', 'NOT', 'NOT',\n",
       "       'HOF', 'NOT', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF',\n",
       "       'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'HOF', 'HOF', 'HOF', 'HOF',\n",
       "       'NOT', 'HOF', 'HOF', 'HOF', 'NOT', 'NOT', 'NOT', 'NOT', 'NOT',\n",
       "       'HOF'], dtype=object)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2b4901d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "952d689b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      HOF\n",
       "1      HOF\n",
       "2      NOT\n",
       "3      HOF\n",
       "4      NOT\n",
       "      ... \n",
       "995    NOT\n",
       "996    NOT\n",
       "997    NOT\n",
       "998    NOT\n",
       "999    HOF\n",
       "Name: task1, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['task1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "58a5cee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         HOF       0.99      1.00      1.00       501\n",
      "         NOT       1.00      0.99      1.00       499\n",
      "\n",
      "    accuracy                           1.00      1000\n",
      "   macro avg       1.00      1.00      1.00      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(data['task1'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a5afe880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         HOF       0.85      0.82      0.83       501\n",
      "         NOT       0.83      0.85      0.84       499\n",
      "\n",
      "    accuracy                           0.84      1000\n",
      "   macro avg       0.84      0.84      0.84      1000\n",
      "weighted avg       0.84      0.84      0.84      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train_tf, data['task1'])\n",
    "naive_bayes_pred=naive_bayes_classifier.predict(X_test_tf)\n",
    "print(classification_report(data['task1'], naive_bayes_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a798d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d27eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
