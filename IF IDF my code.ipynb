{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "386d7769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT DATA</th>\n",
       "      <th>LABELS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nenga rendu perume made for each other llife l...</td>\n",
       "      <td>Non stressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ஆனால் இந்த மாதிரி கேள்விகளை அம்மா, நாத்தனார், ...</td>\n",
       "      <td>Non stressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>correct bro</td>\n",
       "      <td>Non stressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paakka paakka echi ooruthu</td>\n",
       "      <td>Non stressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Endha video va pathu like aprm comments la pan...</td>\n",
       "      <td>Non stressed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           TEXT DATA        LABELS\n",
       "0  Nenga rendu perume made for each other llife l...  Non stressed\n",
       "1  ஆனால் இந்த மாதிரி கேள்விகளை அம்மா, நாத்தனார், ...  Non stressed\n",
       "2                                        correct bro  Non stressed\n",
       "3                         Paakka paakka echi ooruthu  Non stressed\n",
       "4  Endha video va pathu like aprm comments la pan...  Non stressed"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Importing required module\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import  word_tokenize \n",
    " \n",
    "#Example text corpus for our tutorial\n",
    "data=pd.read_csv('TAMIL_DEVELOPMENT_DATA.csv')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a2d0cf3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    for sent in data:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing the text data\n",
    "def myfunct():\n",
    "        sentences = []\n",
    "        word_set = []\n",
    " \n",
    "    for sent in data:\n",
    "        x = [i.lower() for  i in word_tokenize(sent) if i.isalpha()]\n",
    "            sentences.append(x)\n",
    "\n",
    "        for word in x:\n",
    "            if word not in word_set:\n",
    "                word_set.append(word)\n",
    " \n",
    "\n",
    "# Set of vocab \n",
    "word_set = set(word_set)\n",
    "#Total documents in our corpus\n",
    "total_documents = len(sentences)\n",
    " \n",
    "#Creating an index for each word in our vocab.\n",
    "index_dict = {} #Dictionary to store index for each word\n",
    "i = 0\n",
    "for word in word_set:\n",
    "    index_dict[word] = i\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "22c84e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the text data\n",
    "def myfunct():\n",
    "    sentences = []\n",
    "    word_set = []\n",
    "\n",
    "    for sent in data:\n",
    "        x = [i.lower() for i in word_tokenize(sent) if i.isalpha()]\n",
    "        sentences.append(x)\n",
    "        for word in x:\n",
    "            if word not in word_set:\n",
    "                word_set.append(word)\n",
    "\n",
    "    # Set of vocab\n",
    "    word_set = set(word_set)\n",
    "    # Total documents in our corpus\n",
    "    total_documents = len(sentences)\n",
    "\n",
    "    # Creating an index for each word in our vocab.\n",
    "    index_dict = {}  # Dictionary to store index for each word\n",
    "    i = 0\n",
    "    for word in word_set:\n",
    "        index_dict[word] = i\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "21a1a5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "1    None\n",
       "2    None\n",
       "3    None\n",
       "4    None\n",
       "Name: TEXT DATA, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['l1']=data['TEXT DATA'][:10].apply(lambda X: myfunct())\n",
    "pre_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a41955fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a count dictionary\n",
    " \n",
    "def count_dict(sentences):\n",
    "    word_count = {}\n",
    "    for word in word_set:\n",
    "        word_count[word] = 0\n",
    "        for sent in sentences:\n",
    "            if word in sent:\n",
    "                word_count[word] += 1\n",
    "    return word_count\n",
    " \n",
    "word_count = count_dict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "53d6acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term Frequency\n",
    "def termfreq(document, word):\n",
    "    N = len(document)\n",
    "    occurance = len([token for token in document if token == word])\n",
    "    return occurance/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "38850962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['topic', 'sentences', 'are', 'similar', 'to', 'mini', 'thesis', 'statements', 'like', 'a', 'thesis', 'statement', 'a', 'topic', 'sentence', 'has', 'a', 'specific', 'main', 'point', 'whereas', 'the', 'thesis', 'is', 'the', 'main', 'point', 'of', 'the', 'essay'], ['the', 'topic', 'sentence', 'is', 'the', 'main', 'point', 'of', 'the', 'paragraph', 'like', 'the', 'thesis', 'statement', 'a', 'topic', 'sentence', 'has', 'a', 'unifying', 'function', 'but', 'a', 'thesis', 'statement', 'or', 'topic', 'sentence', 'alone', 'doesn', 't', 'guarantee', 'unity'], ['an', 'essay', 'is', 'unified', 'if', 'all', 'the', 'paragraphs', 'relate', 'to', 'the', 'thesis', 'whereas', 'a', 'paragraph', 'is', 'unified', 'if', 'all', 'the', 'sentences', 'relate', 'to', 'the', 'topic', 'sentence']]\n"
     ]
    }
   ],
   "source": [
    "#Importing required module\n",
    "import numpy as np\n",
    "from nltk.tokenize import  word_tokenize \n",
    " \n",
    "#Example text corpus for our tutorial\n",
    "text = ['Topic sentences are similar to mini thesis statements.\\\n",
    "        Like a thesis statement, a topic sentence has a specific \\\n",
    "        main point. Whereas the thesis is the main point of the essay',\\\n",
    "        'the topic sentence is the main point of the paragraph.\\\n",
    "        Like the thesis statement, a topic sentence has a unifying function. \\\n",
    "        But a thesis statement or topic sentence alone doesn’t guarantee unity.', \\\n",
    "        'An essay is unified if all the paragraphs relate to the thesis,\\\n",
    "        whereas a paragraph is unified if all the sentences relate to the topic sentence.']\n",
    " \n",
    "#Preprocessing the text data\n",
    "sentences = []\n",
    "word_set = []\n",
    " \n",
    "for sent in text:\n",
    "    x = [i.lower() for  i in word_tokenize(sent) if i.isalpha()]\n",
    "    sentences.append(x)\n",
    "    for word in x:\n",
    "        if word not in word_set:\n",
    "            word_set.append(word)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a4d51f97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n",
      "e\n",
      "l\n",
      "l\n",
      "o\n",
      " \n",
      "k\n",
      "a\n",
      "v\n",
      "y\n",
      "a\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      " \n",
      "r\n",
      " \n",
      "k\n",
      "a\n",
      "v\n",
      "y\n",
      "a\n",
      " \n"
     ]
    }
   ],
   "source": [
    "a=\"hello kavya you r kavya \"\n",
    "word=\"kavya\"\n",
    "for i in a:\n",
    "    print (i)\n",
    "    \n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "007f0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverse Document Frequency\n",
    " \n",
    "def inverse_doc_freq(word):\n",
    "    try:\n",
    "        word_occurance = word_count[word] + 1\n",
    "    except:\n",
    "        word_occurance = 1\n",
    "    return np.log(total_documents/word_occurance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1744ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(sentence):\n",
    "    tf_idf_vec = np.zeros((len(word_set),))\n",
    "    for word in sentence:\n",
    "        tf = termfreq(sentence,word)\n",
    "        idf = inverse_doc_freq(word)\n",
    "         \n",
    "        value = tf*idf\n",
    "        tf_idf_vec[index_dict[word]] = value \n",
    "    return tf_idf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b40dbe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Encoded text corpus\n",
    "vectors = []\n",
    "for sent in sentences:\n",
    "    vec = tf_idf(sent)\n",
    "    vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f4ef170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "print(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "841e9399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required module\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fa45506d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT DATA</th>\n",
       "      <th>LABELS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nenga rendu perume made for each other llife l...</td>\n",
       "      <td>Non stressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ஆனால் இந்த மாதிரி கேள்விகளை அம்மா, நாத்தனார், ...</td>\n",
       "      <td>Non stressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>correct bro</td>\n",
       "      <td>Non stressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paakka paakka echi ooruthu</td>\n",
       "      <td>Non stressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Endha video va pathu like aprm comments la pan...</td>\n",
       "      <td>Non stressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>என் பெயர் கேபி, எனக்கு 18 வயது, நா இப்போது சிற...</td>\n",
       "      <td>stressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>yAro ! : யாராவது எனக்கு உதவ muDhiyumA, நா பேச ...</td>\n",
       "      <td>stressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>நா iனRu செர்ட்ராலைனில் வைக்கப்பட்டேன்: எப்படி ...</td>\n",
       "      <td>stressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>marundhu கொணர்வி : என் மருந்தை மாற்ற veNDhuM è...</td>\n",
       "      <td>stressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>IM SO FUCKED IM SO FUCKED IM SO FUUUUCK FFUUCK...</td>\n",
       "      <td>stressed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1378 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              TEXT DATA        LABELS\n",
       "0     Nenga rendu perume made for each other llife l...  Non stressed\n",
       "1     ஆனால் இந்த மாதிரி கேள்விகளை அம்மா, நாத்தனார், ...  Non stressed\n",
       "2                                           correct bro  Non stressed\n",
       "3                            Paakka paakka echi ooruthu  Non stressed\n",
       "4     Endha video va pathu like aprm comments la pan...  Non stressed\n",
       "...                                                 ...           ...\n",
       "1373  என் பெயர் கேபி, எனக்கு 18 வயது, நா இப்போது சிற...      stressed\n",
       "1374  yAro ! : யாராவது எனக்கு உதவ muDhiyumA, நா பேச ...      stressed\n",
       "1375  நா iனRu செர்ட்ராலைனில் வைக்கப்பட்டேன்: எப்படி ...      stressed\n",
       "1376  marundhu கொணர்வி : என் மருந்தை மாற்ற veNDhuM è...      stressed\n",
       "1377  IM SO FUCKED IM SO FUCKED IM SO FUUUUCK FFUUCK...      stressed\n",
       "\n",
       "[1378 rows x 2 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example text corpus for our tutorial\n",
    "data = pd.read_csv('TAMIL_DEVELOPMENT_DATA.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fa5cdd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the text data\n",
    "def preprocess_text(text):\n",
    "    return [word.lower() for word in word_tokenize(text) if word.isalpha()]\n",
    "\n",
    "def myfunct(data):\n",
    "    sentences = []\n",
    "    word_set = []\n",
    "\n",
    "    for sent in data['TEXT DATA']:\n",
    "        x = preprocess_text(sent)\n",
    "        sentences.append(x)\n",
    "        for word in x:\n",
    "            if word not in word_set:\n",
    "                word_set.append(word)\n",
    "\n",
    "    # Set of vocab\n",
    "    word_set = set(word_set)\n",
    "    # Total documents in our corpus\n",
    "    total_documents = len(sentences)\n",
    "\n",
    "    # Creating an index for each word in our vocab.\n",
    "    index_dict = {}  # Dictionary to store index for each word\n",
    "    i = 0\n",
    "    for word in word_set:\n",
    "        index_dict[word] = i\n",
    "        i += 1\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "967d6a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           TEXT DATA        LABELS  \\\n",
      "0  Nenga rendu perume made for each other llife l...  Non stressed   \n",
      "1  ஆனால் இந்த மாதிரி கேள்விகளை அம்மா, நாத்தனார், ...  Non stressed   \n",
      "2                                        correct bro  Non stressed   \n",
      "3                         Paakka paakka echi ooruthu  Non stressed   \n",
      "4  Endha video va pathu like aprm comments la pan...  Non stressed   \n",
      "5  STAR777 ஆம் தம்பி, உங்களைப் போன்றோர் இருக்கும்...  Non stressed   \n",
      "6  When we understand each other nd solve the pro...  Non stressed   \n",
      "7  I remember listening this song in i pod of my ...  Non stressed   \n",
      "8   @kirthiraaj0  starting problem basic educatio...  Non stressed   \n",
      "9                    aandavaru jagajjaala killadi pa  Non stressed   \n",
      "\n",
      "                                                  l1  \n",
      "0  [nenga, rendu, perume, made, for, each, other,...  \n",
      "1                                                 []  \n",
      "2                                     [correct, bro]  \n",
      "3                    [paakka, paakka, echi, ooruthu]  \n",
      "4  [endha, video, va, pathu, like, aprm, comments...  \n",
      "5                                                 []  \n",
      "6  [when, we, understand, each, other, nd, solve,...  \n",
      "7  [i, remember, listening, this, song, in, i, po...  \n",
      "8  [starting, problem, basic, education, than, av...  \n",
      "9               [aandavaru, jagajjaala, killadi, pa]  \n"
     ]
    }
   ],
   "source": [
    "data['l1'] = myfunct(data)\n",
    "pre_data = data.head(10)  # Assuming you want to display the first 10 rows of the processed data\n",
    "print(pre_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9f1d1232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term Frequency\n",
    "def termfreq(document, word):\n",
    "    N = len(document)\n",
    "    occurance = len([token for token in document if token == word])\n",
    "    return occurance/N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b1a785cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "termfreq(pre_data,\"each\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16c5664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
