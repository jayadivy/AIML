{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "170f5dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d4a37a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d7851e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1123757263427186690</td>\n",
       "      <td>hate wen females hit ah nigga with tht bro ðŸ˜‚ðŸ˜‚,...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1123733301397733380</td>\n",
       "      <td>RT @airjunebug: When you're from the Bay but y...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1123734094108659712</td>\n",
       "      <td>RT @DonaldJTrumpJr: Dear Democrats: The Americ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1126951188170199049</td>\n",
       "      <td>RT @SheLoveTimothy: He ainâ€™t on drugs he just ...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1126863510447710208</td>\n",
       "      <td>RT @TavianJordan: Summer â€˜19 Iâ€™m coming for yo...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1126798721025544193</td>\n",
       "      <td>RT @prodnose: Good morning, everyone.\\nFollowi...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1126833089190219777</td>\n",
       "      <td>@cheezitking123 this what you get for tryna ge...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1130037092845670400</td>\n",
       "      <td>earphones ko ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1127028455651123201</td>\n",
       "      <td>RT @nj_linguist: @realgonegirl @elivalley I th...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1130285076858789889</td>\n",
       "      <td>iâ€™m tired as fuck. and man, physically ainâ€™t S...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id                                               text  \\\n",
       "0    1123757263427186690  hate wen females hit ah nigga with tht bro ðŸ˜‚ðŸ˜‚,...   \n",
       "1    1123733301397733380  RT @airjunebug: When you're from the Bay but y...   \n",
       "2    1123734094108659712  RT @DonaldJTrumpJr: Dear Democrats: The Americ...   \n",
       "3    1126951188170199049  RT @SheLoveTimothy: He ainâ€™t on drugs he just ...   \n",
       "4    1126863510447710208  RT @TavianJordan: Summer â€˜19 Iâ€™m coming for yo...   \n",
       "..                   ...                                                ...   \n",
       "995  1126798721025544193  RT @prodnose: Good morning, everyone.\\nFollowi...   \n",
       "996  1126833089190219777  @cheezitking123 this what you get for tryna ge...   \n",
       "997  1130037092845670400                               earphones ko ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­   \n",
       "998  1127028455651123201  RT @nj_linguist: @realgonegirl @elivalley I th...   \n",
       "999  1130285076858789889  iâ€™m tired as fuck. and man, physically ainâ€™t S...   \n",
       "\n",
       "    task1  \n",
       "0     HOF  \n",
       "1     HOF  \n",
       "2     NOT  \n",
       "3     HOF  \n",
       "4     NOT  \n",
       "..    ...  \n",
       "995   NOT  \n",
       "996   NOT  \n",
       "997   NOT  \n",
       "998   NOT  \n",
       "999   HOF  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('sample_text - Sheet1 (1).csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29e50f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emot in c:\\users\\user\\anaconda3\\lib\\site-packages (3.1)\n",
      "Requirement already satisfied: emoji in c:\\users\\user\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: demoji in c:\\users\\user\\anaconda3\\lib\\site-packages (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement punct (from versions: none)\n",
      "ERROR: No matching distribution found for punct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\user\\anaconda3\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\user\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\user\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "Requirement already satisfied: anyascii in c:\\users\\user\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install emot\n",
    "!pip install emoji\n",
    "!pip install demoji\n",
    "!pip install punct\n",
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "530a83c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5809e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2e381fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_18684/1435932555.py:23: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    }
   ],
   "source": [
    "#from textblob import TextBlob\n",
    "import emot\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import emoji\n",
    "import demoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import regex\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "demoji.download_codes()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be60b4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1123757263427186690</td>\n",
       "      <td>hate wen females hit ah nigga with tht bro ðŸ˜‚ðŸ˜‚,...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1123733301397733380</td>\n",
       "      <td>RT @airjunebug: When you're from the Bay but y...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1123734094108659712</td>\n",
       "      <td>RT @DonaldJTrumpJr: Dear Democrats: The Americ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1126951188170199049</td>\n",
       "      <td>RT @SheLoveTimothy: He ainâ€™t on drugs he just ...</td>\n",
       "      <td>HOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1126863510447710208</td>\n",
       "      <td>RT @TavianJordan: Summer â€˜19 Iâ€™m coming for yo...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1123757263427186690  hate wen females hit ah nigga with tht bro ðŸ˜‚ðŸ˜‚,...   \n",
       "1  1123733301397733380  RT @airjunebug: When you're from the Bay but y...   \n",
       "2  1123734094108659712  RT @DonaldJTrumpJr: Dear Democrats: The Americ...   \n",
       "3  1126951188170199049  RT @SheLoveTimothy: He ainâ€™t on drugs he just ...   \n",
       "4  1126863510447710208  RT @TavianJordan: Summer â€˜19 Iâ€™m coming for yo...   \n",
       "\n",
       "  task1  \n",
       "0   HOF  \n",
       "1   HOF  \n",
       "2   NOT  \n",
       "3   HOF  \n",
       "4   NOT  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=data['text']\n",
    "y=data['task1']\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5e1316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b8e7764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HOF    501\n",
       "NOT    499\n",
       "Name: task1, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['task1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "966c71be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emot_object = emot.core.emot()\n",
    "ps =PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "english_stopwords = stopwords.words('english')\n",
    "exclude = set(string.punctuation)\n",
    "def preprocess(text):\n",
    "  #text=demoji.findall(df['Text'])\n",
    "    text = contractions.fix(text.lower(), slang=True)\n",
    "    text =re.sub(\"@ ?[A-Za-z0-9_]+\", \"\", text)\n",
    "    text= re.sub(r'\\d+', '', text)\n",
    "    text=re.sub(r'$', '', text)\n",
    "    text= re.sub(r'â€™','', text )\n",
    "    text=re.sub('<.*?>','',text)\n",
    "    text=re.sub(r'http\\S+', '', text)\n",
    "  #text=emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    text = ''.join(ch for ch in text if ch not in exclude)\n",
    "    tokens = word_tokenize(text)\n",
    "  #print(\"Tokens:\", tokens)\n",
    "    text = [t for t in tokens if t not in english_stopwords]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f53fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "#import demoji\n",
    "#demoji.download_codes()\n",
    "def emo(text):\n",
    "    \n",
    "    temp=emoji.demojize(text,delimiters=(\" \",\" \"))\n",
    "    temp=temp.replace(\"_\",\"  \")\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c0b9daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['clean_text']=data[\"text\"].apply(lambda x:emo(x))\n",
    "data[\"clean_text\"]=data['clean_text'].apply(lambda X: preprocess(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0698194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1123757263427186690</td>\n",
       "      <td>hate wen females hit ah nigga with tht bro ðŸ˜‚ðŸ˜‚,...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>hate wen females hit ah nigga tht bro face tea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1123733301397733380</td>\n",
       "      <td>RT @airjunebug: When you're from the Bay but y...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>rt bay really ny nigga heart w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1123734094108659712</td>\n",
       "      <td>RT @DonaldJTrumpJr: Dear Democrats: The Americ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>rt dear democrats american people stupid know ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1126951188170199049</td>\n",
       "      <td>RT @SheLoveTimothy: He ainâ€™t on drugs he just ...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>rt drugs bored shit bored face tears joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1126863510447710208</td>\n",
       "      <td>RT @TavianJordan: Summer â€˜19 Iâ€™m coming for yo...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>rt summer â€˜ coming boring shit beach days road...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1126798721025544193</td>\n",
       "      <td>RT @prodnose: Good morning, everyone.\\nFollowi...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>rt good morning everyone following one worst d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1126833089190219777</td>\n",
       "      <td>@cheezitking123 this what you get for tryna ge...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>get tryna get kfc expressionless face expressi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1130037092845670400</td>\n",
       "      <td>earphones ko ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­</td>\n",
       "      <td>NOT</td>\n",
       "      <td>earphones ko loudly crying face loudly crying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1127028455651123201</td>\n",
       "      <td>RT @nj_linguist: @realgonegirl @elivalley I th...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>rt linguist think people need realize art ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1130285076858789889</td>\n",
       "      <td>iâ€™m tired as fuck. and man, physically ainâ€™t S...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>tired fuck man physically shit mentally draine...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id                                               text  \\\n",
       "0    1123757263427186690  hate wen females hit ah nigga with tht bro ðŸ˜‚ðŸ˜‚,...   \n",
       "1    1123733301397733380  RT @airjunebug: When you're from the Bay but y...   \n",
       "2    1123734094108659712  RT @DonaldJTrumpJr: Dear Democrats: The Americ...   \n",
       "3    1126951188170199049  RT @SheLoveTimothy: He ainâ€™t on drugs he just ...   \n",
       "4    1126863510447710208  RT @TavianJordan: Summer â€˜19 Iâ€™m coming for yo...   \n",
       "..                   ...                                                ...   \n",
       "995  1126798721025544193  RT @prodnose: Good morning, everyone.\\nFollowi...   \n",
       "996  1126833089190219777  @cheezitking123 this what you get for tryna ge...   \n",
       "997  1130037092845670400                               earphones ko ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­   \n",
       "998  1127028455651123201  RT @nj_linguist: @realgonegirl @elivalley I th...   \n",
       "999  1130285076858789889  iâ€™m tired as fuck. and man, physically ainâ€™t S...   \n",
       "\n",
       "    task1                                         clean_text  \n",
       "0     HOF  hate wen females hit ah nigga tht bro face tea...  \n",
       "1     HOF                     rt bay really ny nigga heart w  \n",
       "2     NOT  rt dear democrats american people stupid know ...  \n",
       "3     HOF           rt drugs bored shit bored face tears joy  \n",
       "4     NOT  rt summer â€˜ coming boring shit beach days road...  \n",
       "..    ...                                                ...  \n",
       "995   NOT  rt good morning everyone following one worst d...  \n",
       "996   NOT  get tryna get kfc expressionless face expressi...  \n",
       "997   NOT  earphones ko loudly crying face loudly crying ...  \n",
       "998   NOT  rt linguist think people need realize art ever...  \n",
       "999   HOF  tired fuck man physically shit mentally draine...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f72cc6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import regex\n",
    "\n",
    "def custom_analyzer(text):\n",
    "    words = regex.findall(r'\\w{2,}', text) # extract words of at least 2 letters\n",
    "    for w in words:\n",
    "        yield w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e35344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf1 = CountVectorizer(analyzer='word', ngram_range=(1, 3))\n",
    "#applying tf idf to training data\n",
    "X_train_tf1 = tf_idf1.fit_transform(data['clean_text'])\n",
    "#applying tf idf to training data\n",
    "X_train_tf1 = tf_idf1.transform(data['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1dbc5c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 219)\t2\n",
      "  (0, 220)\t1\n",
      "  (0, 231)\t1\n",
      "  (0, 232)\t1\n",
      "  (0, 1860)\t2\n",
      "  (0, 1863)\t1\n",
      "  (0, 1864)\t1\n",
      "  (0, 4322)\t2\n",
      "  (0, 4407)\t2\n",
      "  (0, 4408)\t2\n",
      "  (0, 4635)\t1\n",
      "  (0, 4636)\t1\n",
      "  (0, 4637)\t1\n",
      "  (0, 5046)\t1\n",
      "  (0, 5047)\t1\n",
      "  (0, 5048)\t1\n",
      "  (0, 6525)\t1\n",
      "  (0, 6553)\t1\n",
      "  (0, 6554)\t1\n",
      "  (0, 6780)\t1\n",
      "  (0, 6781)\t1\n",
      "  (0, 6782)\t1\n",
      "  (0, 7498)\t2\n",
      "  (0, 7505)\t1\n",
      "  (0, 7506)\t1\n",
      "  :\t:\n",
      "  (999, 3588)\t1\n",
      "  (999, 3624)\t1\n",
      "  (999, 3625)\t1\n",
      "  (999, 3626)\t1\n",
      "  (999, 5046)\t1\n",
      "  (999, 5141)\t1\n",
      "  (999, 5142)\t1\n",
      "  (999, 5240)\t1\n",
      "  (999, 5272)\t1\n",
      "  (999, 5273)\t1\n",
      "  (999, 8822)\t1\n",
      "  (999, 8843)\t1\n",
      "  (999, 8844)\t1\n",
      "  (999, 9121)\t1\n",
      "  (999, 9122)\t1\n",
      "  (999, 9123)\t1\n",
      "  (999, 10766)\t1\n",
      "  (999, 10767)\t1\n",
      "  (999, 10768)\t1\n",
      "  (999, 13451)\t1\n",
      "  (999, 13529)\t1\n",
      "  (999, 13530)\t1\n",
      "  (999, 15347)\t1\n",
      "  (999, 15348)\t1\n",
      "  (999, 15349)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a52a92c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming test data into tf-idf matrix\n",
    "X_test_tf1 = tf_idf1.transform(data[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40ff1abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         HOF       0.98      1.00      0.99       501\n",
      "         NOT       1.00      0.98      0.99       499\n",
      "\n",
      "    accuracy                           0.99      1000\n",
      "   macro avg       0.99      0.99      0.99      1000\n",
      "weighted avg       0.99      0.99      0.99      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train_tf1, data['task1'])\n",
    "naive_bayes_pred=naive_bayes_classifier.predict(X_test_tf1)\n",
    "print(classification_report(data['task1'], naive_bayes_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d16a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4303c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2a4597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f986a70d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
