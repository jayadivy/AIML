{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8057eeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emot in c:\\users\\user\\anaconda3\\lib\\site-packages (3.1)\n",
      "Requirement already satisfied: emoji in c:\\users\\user\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: demoji in c:\\users\\user\\anaconda3\\lib\\site-packages (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement punct (from versions: none)\n",
      "ERROR: No matching distribution found for punct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\user\\anaconda3\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\user\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\user\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "Requirement already satisfied: anyascii in c:\\users\\user\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.15.0-cp39-cp39-win_amd64.whl (2.1 kB)\n",
      "Collecting tensorflow-intel==2.15.0\n",
      "  Using cached tensorflow_intel-2.15.0-cp39-cp39-win_amd64.whl (300.8 MB)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.12.1)\n",
      "Collecting tensorboard<2.16,>=2.15\n",
      "  Using cached tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting keras<2.16,>=2.15.0\n",
      "  Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.26.2)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0\n",
      "  Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.4.0)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (58.0.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.20.3)\n",
      "Collecting ml-dtypes~=0.2.0\n",
      "  Using cached ml_dtypes-0.2.0-cp39-cp39-win_amd64.whl (938 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.2.1)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.59.3-cp39-cp39-win_amd64.whl (3.7 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.5.1-py3-none-any.whl (102 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.24.0)\n",
      "Collecting google-auth-oauthlib<2,>=0.5\n",
      "  Using cached google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Installing collected packages: markdown, grpcio, google-auth-oauthlib, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, ml-dtypes, libclang, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.0.0\n",
      "    Uninstalling keras-3.0.0:\n",
      "      Successfully uninstalled keras-3.0.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 23.1.21\n",
      "    Uninstalling flatbuffers-23.1.21:\n",
      "      Successfully uninstalled flatbuffers-23.1.21\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-23.5.26 gast-0.5.4 google-auth-oauthlib-1.1.0 google-pasta-0.2.0 grpcio-1.59.3 keras-2.15.0 libclang-16.0.6 markdown-3.5.1 ml-dtypes-0.2.0 opt-einsum-3.3.0 tensorboard-2.15.1 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-intel-2.15.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0\n",
      "Requirement already satisfied: keras in c:\\users\\user\\anaconda3\\lib\\site-packages (2.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install emot\n",
    "!pip install emoji\n",
    "!pip install demoji\n",
    "!pip install punct\n",
    "!pip install contractions\n",
    "!pip install tensorflow\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cfeeeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c583813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c2447e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n",
    "pd.set_option(\"max_colwidth\" ,220)\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8e87cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_8204/2799970465.py:23: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    }
   ],
   "source": [
    "#from textblob import TextBlob\n",
    "import emoji\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import emoji\n",
    "import demoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import regex\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "demoji.download_codes()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "670bb091",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\user\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.59.3)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (58.0.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.26.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: keras in c:\\users\\user\\anaconda3\\lib\\site-packages (2.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0135a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#import xgboost as xgb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a05c8f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('Fake_train.csv')\n",
    "df_dev=pd.read_csv('Fake_dev.csv')\n",
    "#df_test=pd.read_csv('/content/drive/MyDrive/AI ML/Fake_test_without_labels.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2a58b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>നല്ല അവതരണം. സത്യം പുറത്തു വരട്ടെ</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Masha Allah</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>അന്വേഷണം കഴിയുമ്പോൾ,. C. A. A. യ്ക്ക് എതിരായ കലാപം പോലെ ആകുമോ.. സ്വന്തം ആളെ തന്നെ പിടിച്ചു അകത്തിടേണ്ടി വരുമോ?</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Illathentha avaru purath vidayittalland verenth</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barana pakshathin matoru niyamam.nalla moyanth manthrii</td>\n",
       "      <td>original</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                             text  \\\n",
       "0                                                                               നല്ല അവതരണം. സത്യം പുറത്തു വരട്ടെ   \n",
       "1                                                                                                     Masha Allah   \n",
       "2  അന്വേഷണം കഴിയുമ്പോൾ,. C. A. A. യ്ക്ക് എതിരായ കലാപം പോലെ ആകുമോ.. സ്വന്തം ആളെ തന്നെ പിടിച്ചു അകത്തിടേണ്ടി വരുമോ?   \n",
       "3                                                                 Illathentha avaru purath vidayittalland verenth   \n",
       "4                                                         Barana pakshathin matoru niyamam.nalla moyanth manthrii   \n",
       "\n",
       "      label  \n",
       "0      Fake  \n",
       "1      Fake  \n",
       "2      Fake  \n",
       "3      Fake  \n",
       "4  original  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c467630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Full.  Musilm.   Verodamum</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>പക്ഷികളും മൃഗങ്ങളും ഈ ലോകത്ത് സുഖമായി ജീവിക്കുന്നു വിവരവും വിദ്യാഭ്യാസവും ഉണ്ട് എന്ന് പറയുന്ന മനുഷ്യർ മരമണ്ടൻ മാരായി ജീവിക്കുന്നു മോഹനൻ വൈദ്യരും ജേക്കബ് വടക്കഞ്ചേരി യും പറയുന്ന തലങ്ങളിലേക്ക് നിങ്ങൾ എത്താതെ തന്നെ അവരെ...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ഒരു താടിക്കാരൻ പാത്രം കൊട്ടാൻ പറഞ്ഞപ്പോ ........ മറ്റൊരു ഒന്നും ഇല്ലാത്തവൻ കൈ കൊട്ടാൻ പറയുന്നു ....... 😭😭😭😭</td>\n",
       "      <td>original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>കുംഭളേമ&lt;br&gt;മറന്നോ</td>\n",
       "      <td>original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ഇത് തിരുവാതിര അല്ല...... കോറോണയെ കൈകൊട്ടി കൊല്ലുകയാണ് 😂😂😂</td>\n",
       "      <td>original</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                          text  \\\n",
       "0                                                                                                                                                                                                   Full.  Musilm.   Verodamum   \n",
       "1  പക്ഷികളും മൃഗങ്ങളും ഈ ലോകത്ത് സുഖമായി ജീവിക്കുന്നു വിവരവും വിദ്യാഭ്യാസവും ഉണ്ട് എന്ന് പറയുന്ന മനുഷ്യർ മരമണ്ടൻ മാരായി ജീവിക്കുന്നു മോഹനൻ വൈദ്യരും ജേക്കബ് വടക്കഞ്ചേരി യും പറയുന്ന തലങ്ങളിലേക്ക് നിങ്ങൾ എത്താതെ തന്നെ അവരെ...   \n",
       "2                                                                                                                  ഒരു താടിക്കാരൻ പാത്രം കൊട്ടാൻ പറഞ്ഞപ്പോ ........ മറ്റൊരു ഒന്നും ഇല്ലാത്തവൻ കൈ കൊട്ടാൻ പറയുന്നു ....... 😭😭😭😭   \n",
       "3                                                                                                                                                                                                            കുംഭളേമ<br>മറന്നോ   \n",
       "4                                                                                                                                                                    ഇത് തിരുവാതിര അല്ല...... കോറോണയെ കൈകൊട്ടി കൊല്ലുകയാണ് 😂😂😂   \n",
       "\n",
       "      label  \n",
       "0      Fake  \n",
       "1      Fake  \n",
       "2  original  \n",
       "3  original  \n",
       "4  original  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2edc4ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "original    1658\n",
       "Fake        1599\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17b8c079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "original    409\n",
       "Fake        406\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7155dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emot_object = emot.core.emot()        #PREPROCESSING\n",
    "ps =PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "english_stopwords = stopwords.words('english')\n",
    "exclude = set(string.punctuation)\n",
    "def preprocess(text):\n",
    "    \n",
    "    text = contractions.fix(text.lower(), slang=True)#text=demoji.findall(df['Text'])\n",
    "    text =re.sub(\"@ ?[A-Za-z0-9_]+\", \"\", text)\n",
    "    text= re.sub(r'\\d+', '', text)\n",
    "    text=re.sub(r'$', '', text)\n",
    "    text= re.sub(r'’','', text )\n",
    "    text=re.sub('<.*?>','',text)\n",
    "    text=re.sub(r'http\\S+', '', text)\n",
    "    text = ''.join(ch for ch in text if ch not in exclude)#text=emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    tokens = word_tokenize(text) #print(\"Tokens:\", tokens)\n",
    "    text = [t for t in tokens if t not in english_stopwords]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1f3a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "#import demoji\n",
    "#demoji.download_codes()\n",
    "def emo(text):\n",
    "    temp=emoji.demojize(text,delimiters=(\" \",\" \"))\n",
    "    temp=temp.replace(\"_\",\"  \")\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e649bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['clean_text']=df_train[\"text\"].apply(lambda x:emo(x))\n",
    "df_train[\"clean_text\"]=df_train['clean_text'].apply(lambda X: preprocess(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29e4fdc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>നല്ല അവതരണം. സത്യം പുറത്തു വരട്ടെ</td>\n",
       "      <td>Fake</td>\n",
       "      <td>നല്ല അവതരണം സത്യം പുറത്തു വരട്ടെ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Masha Allah</td>\n",
       "      <td>Fake</td>\n",
       "      <td>masha allah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>അന്വേഷണം കഴിയുമ്പോൾ,. C. A. A. യ്ക്ക് എതിരായ കലാപം പോലെ ആകുമോ.. സ്വന്തം ആളെ തന്നെ പിടിച്ചു അകത്തിടേണ്ടി വരുമോ?</td>\n",
       "      <td>Fake</td>\n",
       "      <td>അന്വേഷണം കഴിയുമ്പോൾ c യ്ക്ക് എതിരായ കലാപം പോലെ ആകുമോ സ്വന്തം ആളെ തന്നെ പിടിച്ചു അകത്തിടേണ്ടി വരുമോ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Illathentha avaru purath vidayittalland verenth</td>\n",
       "      <td>Fake</td>\n",
       "      <td>illathentha avaru purath vidayittalland verenth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barana pakshathin matoru niyamam.nalla moyanth manthrii</td>\n",
       "      <td>original</td>\n",
       "      <td>barana pakshathin matoru niyamamnalla moyanth manthrii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>വീണാ ജോർജ് എന്ന ഒരു ആരോഗ്യ മന്ത്രി വന്നതിനു ശേഷം  കേരളത്തിന്റെ ആരോഗ്യ സ്ഥിതി വളരെ മോശവും പ്രഹസനവുമാണ്...</td>\n",
       "      <td>original</td>\n",
       "      <td>വീണാ ജോർജ് എന്ന ഒരു ആരോഗ്യ മന്ത്രി വന്നതിനു ശേഷം കേരളത്തിന്റെ ആരോഗ്യ സ്ഥിതി വളരെ മോശവും പ്രഹസനവുമാണ്</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3253</th>\n",
       "      <td>ഇതാണ് പിണുവാതിര😂😂😂</td>\n",
       "      <td>original</td>\n",
       "      <td>ഇതാണ് പിണുവാതിര face tears joy face tears joy face tears joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3254</th>\n",
       "      <td>കേസ് എടുക്കണം 💯👍</td>\n",
       "      <td>original</td>\n",
       "      <td>കേസ് എടുക്കണം hundred points thumbs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255</th>\n",
       "      <td>എല്ലാത്തിനേം 501 സോപ്പിട്ട് കളിപ്പിച്ച് വിടണം</td>\n",
       "      <td>original</td>\n",
       "      <td>എല്ലാത്തിനേം സോപ്പിട്ട് കളിപ്പിച്ച് വിടണം</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3256</th>\n",
       "      <td>Day by day leaders r acting like a fool in devil&amp;#39;s on country</td>\n",
       "      <td>original</td>\n",
       "      <td>day day leaders r acting like fool devils country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3257 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                text  \\\n",
       "0                                                                                  നല്ല അവതരണം. സത്യം പുറത്തു വരട്ടെ   \n",
       "1                                                                                                        Masha Allah   \n",
       "2     അന്വേഷണം കഴിയുമ്പോൾ,. C. A. A. യ്ക്ക് എതിരായ കലാപം പോലെ ആകുമോ.. സ്വന്തം ആളെ തന്നെ പിടിച്ചു അകത്തിടേണ്ടി വരുമോ?   \n",
       "3                                                                    Illathentha avaru purath vidayittalland verenth   \n",
       "4                                                            Barana pakshathin matoru niyamam.nalla moyanth manthrii   \n",
       "...                                                                                                              ...   \n",
       "3252        വീണാ ജോർജ് എന്ന ഒരു ആരോഗ്യ മന്ത്രി വന്നതിനു ശേഷം  കേരളത്തിന്റെ ആരോഗ്യ സ്ഥിതി വളരെ മോശവും പ്രഹസനവുമാണ്...   \n",
       "3253                                                                                              ഇതാണ് പിണുവാതിര😂😂😂   \n",
       "3254                                                                                                കേസ് എടുക്കണം 💯👍   \n",
       "3255                                                                   എല്ലാത്തിനേം 501 സോപ്പിട്ട് കളിപ്പിച്ച് വിടണം   \n",
       "3256                                               Day by day leaders r acting like a fool in devil&#39;s on country   \n",
       "\n",
       "         label  \\\n",
       "0         Fake   \n",
       "1         Fake   \n",
       "2         Fake   \n",
       "3         Fake   \n",
       "4     original   \n",
       "...        ...   \n",
       "3252  original   \n",
       "3253  original   \n",
       "3254  original   \n",
       "3255  original   \n",
       "3256  original   \n",
       "\n",
       "                                                                                                clean_text  \n",
       "0                                                                         നല്ല അവതരണം സത്യം പുറത്തു വരട്ടെ  \n",
       "1                                                                                              masha allah  \n",
       "2       അന്വേഷണം കഴിയുമ്പോൾ c യ്ക്ക് എതിരായ കലാപം പോലെ ആകുമോ സ്വന്തം ആളെ തന്നെ പിടിച്ചു അകത്തിടേണ്ടി വരുമോ  \n",
       "3                                                          illathentha avaru purath vidayittalland verenth  \n",
       "4                                                   barana pakshathin matoru niyamamnalla moyanth manthrii  \n",
       "...                                                                                                    ...  \n",
       "3252  വീണാ ജോർജ് എന്ന ഒരു ആരോഗ്യ മന്ത്രി വന്നതിനു ശേഷം കേരളത്തിന്റെ ആരോഗ്യ സ്ഥിതി വളരെ മോശവും പ്രഹസനവുമാണ്  \n",
       "3253                                          ഇതാണ് പിണുവാതിര face tears joy face tears joy face tears joy  \n",
       "3254                                                                   കേസ് എടുക്കണം hundred points thumbs  \n",
       "3255                                                             എല്ലാത്തിനേം സോപ്പിട്ട് കളിപ്പിച്ച് വിടണം  \n",
       "3256                                                     day day leaders r acting like fool devils country  \n",
       "\n",
       "[3257 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fa586f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev['clean_text']=df_dev[\"text\"].apply(lambda x:emo(x))\n",
    "df_dev[\"clean_text\"]=df_dev['clean_text'].apply(lambda X: preprocess(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b2aa310",
   "metadata": {},
   "outputs": [],
   "source": [
    " tf_idf = TfidfVectorizer(max_features=5000)\n",
    "#applying tf idf to training data\n",
    "X_train_tf1 = tf_idf.fit_transform(df_train['clean_text'])\n",
    "#applying tf idf to training data\n",
    "X_train_tf1 = tf_idf.transform(df_train['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "629e1f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming test data into tf-idf matrix\n",
    "X_dev_tf1 = tf_idf.transform(df_dev[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20b824eb",
   "metadata": {},
   "outputs": [],
   "source": [
    " tf_idf1 = TfidfVectorizer()\n",
    "#applying tf idf to training data\n",
    "X_train_tf2 = tf_idf1.fit_transform(df_train['clean_text'])\n",
    "#applying tf idf to training data\n",
    "X_train_tf2 = tf_idf1.transform(df_train['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d12e2821",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_tf2 = tf_idf.transform(df_dev[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30609439",
   "metadata": {},
   "outputs": [],
   "source": [
    " tf_idf1 = TfidfVectorizer(analyzer='word', stop_words=None,ngram_range=(1, 1), max_features=5000)\n",
    "#applying tf idf to training data\n",
    "X_train_tf2 = tf_idf1.fit_transform(df_train['clean_text'])\n",
    "#applying tf idf to training data\n",
    "X_train_tf2 = tf_idf1.transform(df_train['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "023b36b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "bow_representation = vectorizer.fit_transform(df_train['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f9edf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag of Words Representation:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nBag of Words Representation:')\n",
    "print(bow_representation.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac3f759e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag of Words Representation:\n",
      "  (0, 7480)\t1\n",
      "  (0, 6595)\t1\n",
      "  (0, 8236)\t1\n",
      "  (0, 7930)\t1\n",
      "  (0, 8114)\t1\n",
      "  (1, 3453)\t1\n",
      "  (1, 286)\t1\n",
      "  (2, 6553)\t1\n",
      "  (2, 8198)\t1\n",
      "  (2, 6994)\t1\n",
      "  (2, 6822)\t1\n",
      "  (2, 6981)\t1\n",
      "  (2, 6632)\t1\n",
      "  (2, 8096)\t1\n",
      "  (2, 6665)\t1\n",
      "  (2, 7325)\t1\n",
      "  (2, 6527)\t1\n",
      "  (2, 8111)\t1\n",
      "  (3, 2239)\t1\n",
      "  (3, 648)\t1\n",
      "  (3, 4836)\t1\n",
      "  (3, 6273)\t1\n",
      "  (3, 6255)\t1\n",
      "  (4, 758)\t1\n",
      "  (4, 4202)\t1\n",
      "  :\t:\n",
      "  (3252, 8129)\t1\n",
      "  (3252, 7879)\t1\n",
      "  (3252, 7695)\t1\n",
      "  (3252, 8185)\t1\n",
      "  (3252, 8345)\t1\n",
      "  (3252, 7913)\t1\n",
      "  (3253, 1916)\t3\n",
      "  (3253, 5573)\t3\n",
      "  (3253, 2550)\t3\n",
      "  (3253, 6706)\t1\n",
      "  (3254, 6929)\t1\n",
      "  (3254, 6818)\t1\n",
      "  (3254, 5806)\t1\n",
      "  (3254, 2194)\t1\n",
      "  (3254, 4566)\t1\n",
      "  (3255, 6988)\t1\n",
      "  (3255, 6835)\t1\n",
      "  (3255, 7214)\t1\n",
      "  (3256, 3180)\t1\n",
      "  (3256, 3154)\t1\n",
      "  (3256, 1405)\t2\n",
      "  (3256, 1319)\t1\n",
      "  (3256, 1986)\t1\n",
      "  (3256, 1450)\t1\n",
      "  (3256, 135)\t1\n"
     ]
    }
   ],
   "source": [
    "print('\\nBag of Words Representation:')\n",
    "print(bow_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9686d93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train_tf1, df_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2031d3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.82      0.79      0.81       406\n",
      "    original       0.80      0.83      0.82       409\n",
      "\n",
      "    accuracy                           0.81       815\n",
      "   macro avg       0.81      0.81      0.81       815\n",
      "weighted avg       0.81      0.81      0.81       815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB_pred1 = naive_bayes_classifier.predict(X_dev_tf1)\n",
    "print(classification_report(df_dev['label'], NB_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "948e2c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes_classifier.fit(X_train_tf2, df_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbb74a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.82      0.79      0.81       406\n",
      "    original       0.80      0.83      0.82       409\n",
      "\n",
      "    accuracy                           0.81       815\n",
      "   macro avg       0.81      0.81      0.81       815\n",
      "weighted avg       0.81      0.81      0.81       815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB_pred2 = naive_bayes_classifier.predict(X_dev_tf2)\n",
    "print(classification_report(df_dev['label'], NB_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63940c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.81      0.79      0.80       406\n",
      "    original       0.79      0.81      0.80       409\n",
      "\n",
      "    accuracy                           0.80       815\n",
      "   macro avg       0.80      0.80      0.80       815\n",
      "weighted avg       0.80      0.80      0.80       815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_tf1, df_train['label'])\n",
    "LR_pred = lr.predict(X_dev_tf1)\n",
    "print(classification_report(df_dev['label'], LR_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42455bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_traintf=X_train_tf1.toarray()\n",
    "x_testtf=X_dev_tf1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "996ab740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.77      0.53      0.63       406\n",
      "    original       0.64      0.85      0.73       409\n",
      "\n",
      "    accuracy                           0.69       815\n",
      "   macro avg       0.71      0.69      0.68       815\n",
      "weighted avg       0.71      0.69      0.68       815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(x_traintf, df_train['label'])\n",
    "GBN_pred = nb.predict(x_testtf)\n",
    "print(classification_report(df_dev['label'], GBN_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f36fa6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.82      0.77      0.80       406\n",
      "    original       0.79      0.83      0.81       409\n",
      "\n",
      "    accuracy                           0.80       815\n",
      "   macro avg       0.80      0.80      0.80       815\n",
      "weighted avg       0.80      0.80      0.80       815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sv = svm.SVC(kernel='linear')\n",
    "sv.fit(x_traintf, df_train['label'])\n",
    "SVM_pred= sv.predict(x_testtf)\n",
    "print(classification_report(df_dev['label'], SVM_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34186c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_accuracy = 0\n",
    "for x in range(200):\n",
    "    dt = DecisionTreeClassifier(random_state=x)\n",
    "    dt.fit(x_traintf, df_train['label'])\n",
    "    Y_pred_dt = dt.predict(x_testtf)\n",
    "    current_accuracy = round(accuracy_score(Y_pred_dt,df_dev['label'])*100,2)\n",
    "    if(current_accuracy>max_accuracy):\n",
    "        max_accuracy = current_accuracy\n",
    "        best_x = x\n",
    "#print(max_accuracy)\n",
    "#print(best_x)\n",
    "dt = DecisionTreeClassifier(random_state=best_x)\n",
    "dt.fit(x_traintf, df_train['label'])\n",
    "Y_pred_dt = dt.predict(x_testtf)\n",
    "print(classification_report(df_dev['label'], Y_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff650cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_accuracy = 0\n",
    "\n",
    "for x in range(2000):\n",
    "    rf = RandomForestClassifier(random_state=x)\n",
    "    rf.fit(x_traintf, df_train['label'])\n",
    "    Y_pred_rf = rf.predict(x_testtf)\n",
    "    current_accuracy = round(accuracy_score(Y_pred_rf,df_dev['label'])*100,2)\n",
    "    if(current_accuracy>max_accuracy):\n",
    "        max_accuracy = current_accuracy\n",
    "        best_x = x\n",
    "#print(max_accuracy)\n",
    "#print(best_x)\n",
    "rf = RandomForestClassifier(random_state=best_x)\n",
    "rf.fit(x_traintf, df_train['label'])\n",
    "Y_pred_rf = rf.predict(x_testtf)\n",
    "print(classification_report(df_dev['label'], Y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560bc3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
